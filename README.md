# Machine_Learning_Salary_Prediction

## **Introduction**

All project available on: https://www.kaggle.com/code/michalsurmacz/machine-learning-salary-prediction

### **Goal**

The objective of this project is to develop a **predictive model** capable of estimating remuneration for various job positions. By leveraging advanced data analytics and machine learning techniques, the project aims to identify and understand the key factors influencing salary levels. This can provide valuable insights for both employers and employees in the context of market-based pay analytics and salary negotiations.

### **Regression Challenges**

**Regression** is a subset of machine learning algorithms used to predict **continuous values**. Unlike classification, which assigns observations to predefined categories, regression focuses on forecasting numerical values on a continuous scale. Examples of regression applications include predicting real estate prices, sales forecasting, and sports results prediction.

In this project, the regression problem involves predicting the **minimum salary** and the **maximum salary**, a continuous numerical value. Salary prediction is influenced by multiple factors, including **industry**, **company size**, **location**, **professional experience**, and **job position**, making it a complex but insightful problem to solve.

## **Dataset**

### **Source**

The dataset is sourced from [**Kaggle - Salary Prediction**](https://www.kaggle.com/datasets/thedevastator/jobs-dataset-from-glassdoor).

### **Data Description**

The dataset contains the following columns:
- **`Job Id`**: The id of the job. 
- **`Job Title`**: The title of the job position.
- **`Salary Estimate`**: The estimated salary provided by the company for the job.
- **`Job Description`**: A detailed description of the job role.
- **`Rating`**: The company's rating.
- **`Company Name`**: The name of the company offering the job.
- **`Location`**: The location where the job is based.
- **`Headquarters`**: The headquarters of the company.
- **`Size`**: The number of employees in the company.
- **`Founded`**: The year the company was founded.
- **`Type of Ownership`**: The type of ownership of the company (e.g., private, public, government, non-profit).
- **`Industry`**: The industry sector in which the company operates (e.g., Aerospace, Energy).
- **`Sector`**: The specific sector within the industry that the company serves (e.g., Oil & Gas within the Energy industry).
- **`Revenue`**: The total revenue generated by the company.
- **`Competitors`**: The main competitors of the company.

## **Result**

| Model                    | Before Feature Engineering                  |                           |             |            |             |           | After Feature Engineering  |                           |             |            |             |           | After Hyperparameter Tuning |                           |             |            |             |           |
|--------------------------|---------------------------------------------|---------------------------|-------------|------------|-------------|-----------|----------------------------|---------------------------|-------------|------------|-------------|-----------|-----------------------------|---------------------------|-------------|------------|-------------|-----------|
|                          | mae_train                                   | mae_test                  | r2_train    | r2_test    | mape_train  | mape_test | mae_train                  | mae_test                  | r2_train    | r2_test    | mape_train  | mape_test | mae_train                   | mae_test                  | r2_train    | r2_test    | mape_train  | mape_test |
| Linear Regression        | 0.710849                                    | 0.761574                  | 0.192028    | 0.110545   | 3.313348    | 3.034988  | 0.593911                   | 0.588672                  | 0.415575    | 0.426015   | 3.647408    | 3.758545  | 0.593911                    | 0.588672                  | 0.415575    | 0.426015   | 3.647408    | 3.758545  |
| Support Vector Regressor | 0.717460                                    | 0.757966                  | 0.135691    | 0.107117   | 2.386073    | 2.281797  | 0.615417                   | 0.632365                  | 0.310906    | 0.325396   | 2.570187    | 2.795429  | 0.190001                    | 0.373808                  | 0.878802    | 0.648673   | 1.739623    | 3.305663  |
| Random Forest Regressor  | 0.296015                                    | 0.491439                  | 0.774808    | 0.460264   | 1.682999    | 2.629301  | 0.158586                   | 0.344003                  | 0.941386    | 0.717255   | 1.086368    | 1.765511  | 0.158481                    | 0.341285                  | 0.942904    | 0.720477   | 1.110555    | 1.786852  |
| Decision Tree Regressor  | 0.656835                                    | 0.698590                  | 0.262806    | 0.233708   | 2.620582    | 3.247084  | 0.519912                   | 0.631229                  | 0.521396    | 0.335420   | 2.326553    | 2.550005  | 0.298830                    | 0.487718                  | 0.799937    | 0.478919   | 2.614364    | 3.377320  |
| K-Neighbors Regressor    | 0.659543                                    | 0.700953                  | 0.296250    | 0.206386   | 4.023230    | 3.589121  | 0.594237                   | 0.603239                  | 0.403221    | 0.385365   | 3.387811    | 3.443371  | 0.014522                    | 0.257824                  | 0.993414    | 0.719120   | 0.072457    | 0.986110  |


![image](https://github.com/user-attachments/assets/670ff5ad-f8e1-4aed-881b-8f29c9fdfe09)


### **Description**
This machine learning project aims to predict salary estimates using various regression models. The performance of each model is evaluated based on key metrics such as Mean Absolute Error (MAE), R² (Coefficient of Determination), and Mean Absolute Percent Error (MAPE). The impact of feature engineering and hyperparameter tuning on model performance is also assessed.

### **Key Metrics Overview**
- **MAE (Mean Absolute Error)**: Lower values indicate better model performance.
- **R² (Coefficient of Determination)**: Higher values (closer to 1) indicate better model fit.
- **MAPE (Mean Absolute Percent Error)**: Lower values indicate better model performance.

### **Model Performance Comparison**

1. **Linear Regression**
   - **Before Feature Engineering**: High MAE and low R² values, indicating poor predictive performance. High MAPE values suggest significant errors.
   - **After Feature Engineering**: Noticeable improvement in MAE and R², but a slight increase in MAPE.
   - **After Hyperparameter Tuning**: Metrics remained consistent, indicating limited further improvement.

2. **Support Vector Regressor**
   - **Before Feature Engineering**: High MAE and low R² values, indicating poor performance. High MAPE values.
   - **After Feature Engineering**: Improvement in MAE and R², but a slight increase in MAPE.
   - **After Hyperparameter Tuning**: Significant improvement in MAE and R², but MAPE values increased, suggesting a trade-off between bias and variance.

3. **Random Forest Regressor**
   - **Before Feature Engineering**: High MAE and moderate R² values. High MAPE values.
   - **After Feature Engineering**: Significant improvement in MAE and R². MAPE values decreased slightly, indicating better generalization.
   - **After Hyperparameter Tuning**: Metrics remained consistent, indicating robust performance without significant overfitting.

4. **Decision Tree Regressor**
   - **Before Feature Engineering**: High MAE and moderate R² values. High MAPE values.
   - **After Feature Engineering**: Improvement in MAE and R², but a slight increase in MAPE.
   - **After Hyperparameter Tuning**: Some improvement in MAE and R², but MAPE remained high, indicating potential overfitting.

5. **K-Neighbors Regressor**
   - **Before Feature Engineering**: High MAE and moderate R² values. High MAPE values.
   - **After Feature Engineering**: Slight improvement in MAE and R². MAPE values decreased slightly.
   - **After Hyperparameter Tuning**: Significant improvement in MAE and R². MAPE values remained low, indicating good performance without significant overfitting.

### **Summary Insights**
- **Feature Engineering**: Generally leads to improved performance across most models.
- **Hyperparameter Tuning**: Showed the most significant impact on the Support Vector Regressor, achieving the highest R² after tuning.
- **Model Selection**: The Random Forest Regressor and Support Vector Regressor emerged as strong candidates based on test performance after tuning, with Random Forest consistently showing robust performance across all stages.
- **Overfitting Risks**: Notable with K-Neighbors and Decision Tree models, indicating a need for caution in deployment.

### **Recommendations**
- Continue refining features, especially for Linear Regression and Support Vector Regressor.
- Monitor for overfitting, particularly with models showing large discrepancies between training and testing performance.
- Consider ensemble methods or cross-validation techniques for further improvements and robust performance evaluation.

### **Best Model: K-Neighbors Regressor**
- **Performance Metrics**: 
  - MAE (test): 0.26
  - R² (test): 0.72
  - MAPE (test): 0.99
- **Accuracy**: The MAE values for minimum and maximum salary estimates (7.03K USD and 10.4K USD, respectively) indicate good accuracy.
- **Variability**: Standard deviations (27.25K USD for minimum and 40.14K USD for maximum salary estimates) show some variability, but the model makes fairly accurate predictions within these ranges.
- **Reliability**: The relatively low MAE compared to the standard deviations suggests consistent performance across the dataset.

### **Conclusion**
The project successfully demonstrates the importance of feature engineering and hyperparameter tuning in improving model performance. While the Random Forest Regressor and Support Vector Regressor show strong performance, the K-Neighbors Regressor stands out as the best model based on the evaluation metrics. Future work should focus on further refining features, addressing overfitting risks, and exploring ensemble methods for enhanced prediction accuracy.
